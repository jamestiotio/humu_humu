{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUTD 2021 50.007 Machine Learning HMM Project Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\kevin ma\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.19.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Kevin Ma\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Setup and install dependencies\n",
    "!pip3 install numpy\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Enable floating-point underflow warning\n",
    "np.seterr(under=\"warn\")\n",
    "\n",
    "# Set OS-independent paths, relative to current directory\n",
    "es_train_path = os.path.join(\"data\", \"ES\", \"train\")\n",
    "es_dev_in_path = os.path.join(\"data\", \"ES\", \"dev.in\")\n",
    "es_dev_out_path = os.path.join(\"data\", \"ES\", \"dev.out\")\n",
    "es_dev_p1_out_path = os.path.join(\"data\", \"ES\", \"dev.p1.out\")\n",
    "es_dev_p2_out_path = os.path.join(\"data\", \"ES\", \"dev.p2.out\")\n",
    "es_dev_p3_out_path = os.path.join(\"data\", \"ES\", \"dev.p3.out\")\n",
    "ru_train_path = os.path.join(\"data\", \"RU\", \"train\")\n",
    "ru_dev_in_path = os.path.join(\"data\", \"RU\", \"dev.in\")\n",
    "ru_dev_out_path = os.path.join(\"data\", \"RU\", \"dev.out\")\n",
    "ru_dev_p1_out_path = os.path.join(\"data\", \"RU\", \"dev.p1.out\")\n",
    "ru_dev_p2_out_path = os.path.join(\"data\", \"RU\", \"dev.p2.out\")\n",
    "ru_dev_p3_out_path = os.path.join(\"data\", \"RU\", \"dev.p3.out\")\n",
    "\n",
    "# Define constant variables\n",
    "N = 7\n",
    "START, O, BPOS, IPOS, BNEU, INEU, BNEG, INEG, END = 0, 1, 2, 3, 4, 5, 6, 7, 8\n",
    "labels = {\"START\": START,\n",
    "          \"O\": O,\n",
    "          \"B-positive\": BPOS,\n",
    "          \"I-positive\": IPOS,\n",
    "          \"B-neutral\": BNEU,\n",
    "          \"I-neutral\": INEU,\n",
    "          \"B-negative\": BNEG,\n",
    "          \"I-negative\": INEG,\n",
    "          \"END\": END}\n",
    "labels_list = [\"START\", \"O\", \"B-positive\", \"I-positive\", \"B-neutral\", \"I-neutral\", \"B-negative\", \"I-negative\", \"END\"]\n",
    "\n",
    "# Initialise a random number generator with a fixed seed for reproducible results and deterministic behavior\n",
    "rng = np.random.default_rng(1004519 + 1004103 + 1004555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dev.in data\n",
    "def read_dev_in_data(filepath):\n",
    "    results = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            results.append(line.strip())\n",
    "    return results\n",
    "\n",
    "# Read dev.out data\n",
    "def read_dev_out_data(filepath):\n",
    "    results = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            if len(line.strip().rsplit(\" \", 1)) == 2:\n",
    "                token, label = line.strip().rsplit(\" \", 1)\n",
    "                results.append((token, labels[label]))\n",
    "            else:\n",
    "                continue\n",
    "    return results\n",
    "\n",
    "# Read dev.out data with line ending\n",
    "def read_dev_out_data_w_end(filepath):\n",
    "    results = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            if len(line.strip().rsplit(\" \", 1)) == 2:\n",
    "                token, label = line.strip().rsplit(\" \", 1)\n",
    "                results.append((token, labels[label]))\n",
    "            else:\n",
    "                results.append(('', END))\n",
    "                results.append(('', START))\n",
    "    return results\n",
    "\n",
    "# Read training data\n",
    "def read_training_data(filepath):\n",
    "    results = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            if len(line.strip().rsplit(\" \", 1)) == 2:\n",
    "                token, label = line.strip().rsplit(\" \", 1)\n",
    "                results.append((token, labels[label]))\n",
    "            else:\n",
    "                continue\n",
    "    return results\n",
    "\n",
    "# Read training data with line ending\n",
    "def read_training_data_w_end(filepath):\n",
    "    results = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            if len(line.strip().rsplit(\" \", 1)) == 2:\n",
    "                token, label = line.strip().rsplit(\" \", 1)\n",
    "                results.append((token, labels[label]))\n",
    "            else:\n",
    "                results.append(('', END))\n",
    "                results.append(('', START))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_number_of_labels(input_data):\n",
    "    return Counter(elem[1] for elem in input_data)\n",
    "\n",
    "def get_all_unique_tokens(input_data):\n",
    "    # Might want to somehow ensure that this order stays consistent between runs\n",
    "    return list(set(item[0] for item in input_data))\n",
    "\n",
    "# For the return value, we follow the matrix format defined in the slides accordingly\n",
    "def calculate_emission_parameters(input_data, all_unique_tokens, k=1.0):\n",
    "    # Final index is for #UNK# tokens\n",
    "    emission_counts = np.zeros((N, len(all_unique_tokens) + 1), dtype=np.longdouble)\n",
    "\n",
    "    label_counts = np.array(list(val[1] for val in sorted(calculate_number_of_labels(input_data).items())))\n",
    "\n",
    "    for token, label in input_data:\n",
    "        emission_counts[label - 1][all_unique_tokens.index(token)] += 1\n",
    "\n",
    "    # This is for the other case of #UNK# tokens\n",
    "    emission_counts[:, -1] = np.full((1, N), k)[0]\n",
    "\n",
    "    emission_parameters = np.empty((N, len(all_unique_tokens) + 1), dtype=np.longdouble)\n",
    "\n",
    "    for index, _ in enumerate(emission_counts):\n",
    "        emission_parameters[index] = emission_counts[index] / (label_counts[index] + k)\n",
    "\n",
    "    # Do some assertion checks\n",
    "    for row in emission_parameters:\n",
    "        assert np.absolute(1.0 - np.sum(row)) <= np.finfo(np.longdouble).eps\n",
    "\n",
    "    return emission_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tag from word\n",
    "def get_label_from_token(input_word, all_unique_tokens, emission_parameters):\n",
    "    if input_word not in all_unique_tokens:\n",
    "        column_to_consider = emission_parameters[:, -1]\n",
    "    else:\n",
    "        column_to_consider = emission_parameters[:, all_unique_tokens.index(input_word)]\n",
    "\n",
    "    # Randomly choose the index if there is more than one argmax value\n",
    "    x = rng.choice(np.argwhere(np.isclose(column_to_consider, column_to_consider.max())).flatten()) + 1\n",
    "    return labels_list[x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_prediction_output_to_file(language):\n",
    "    if language == \"ES\":\n",
    "        # Conduct training/supervised learning (M-Step)\n",
    "        train_data = read_training_data(es_train_path)\n",
    "        all_unique_tokens = get_all_unique_tokens(train_data)\n",
    "        emission_parameters = calculate_emission_parameters(train_data, all_unique_tokens)\n",
    "\n",
    "        # Execute testing/decoding (E-Step)\n",
    "        predicted_results = []\n",
    "        test_data = read_dev_in_data(es_dev_in_path)\n",
    "        for token in test_data:\n",
    "            if token:\n",
    "                predicted_results.append(token + \" \" + get_label_from_token(token, all_unique_tokens, emission_parameters))\n",
    "            else:\n",
    "                predicted_results.append(\"\")\n",
    "        with open(es_dev_p1_out_path, \"w+\", encoding=\"utf-8\") as file:\n",
    "            for line in predicted_results:\n",
    "                file.write(line + \"\\n\")\n",
    "\n",
    "    elif language == \"RU\":\n",
    "        # Conduct training/supervised learning (M-Step)\n",
    "        train_data = read_training_data(ru_train_path)\n",
    "        all_unique_tokens = get_all_unique_tokens(train_data)\n",
    "        emission_parameters = calculate_emission_parameters(train_data, all_unique_tokens)\n",
    "\n",
    "        # Execute testing/decoding (E-Step)\n",
    "        predicted_results = []\n",
    "        test_data = read_dev_in_data(ru_dev_in_path)\n",
    "        for token in test_data:\n",
    "            if token:\n",
    "                predicted_results.append(token + \" \" + get_label_from_token(token, all_unique_tokens, emission_parameters))\n",
    "            else:\n",
    "                predicted_results.append(\"\")\n",
    "        with open(ru_dev_p1_out_path, \"w+\", encoding=\"utf-8\") as file:\n",
    "            for line in predicted_results:\n",
    "                file.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in [\"ES\", \"RU\"]:\n",
    "    write_prediction_output_to_file(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 Code starting from here\n",
    "\n",
    "def get_emission_from_token(input_word, all_unique_tokens, emission_parameters):\n",
    "    if input_word not in all_unique_tokens:\n",
    "        b = emission_parameters[:, -1]\n",
    "\n",
    "    else:\n",
    "        b = emission_parameters[:, all_unique_tokens.index(input_word)]\n",
    "\n",
    "    return np.reshape(np.pad(b, 1), (-1, 1))\n",
    "\n",
    "def get_parents(res):\n",
    "    # cannot use np.isclose because the scores are really small\n",
    "    return [rng.choice(np.argwhere(res[1:-1, i] == res[1:-1, i].max(axis=0)).flatten()) + 1 for i in range(res.shape[1])]\n",
    "\n",
    "\n",
    "# For the return value, we follow the matrix format defined in the slides accordingly\n",
    "# However, for the convenience of computation later, both END and START appear in both u and v\n",
    "def calculate_transition_parameters(input_data):\n",
    "    transition_counts = np.zeros([N+2, N+2])\n",
    "    priori_counts = np.zeros([N+2, 1])\n",
    "    prev = START\n",
    "    for pair in input_data:\n",
    "        curr = pair[1]\n",
    "        priori_counts[prev] += 1\n",
    "        transition_counts[prev, curr] += 1\n",
    "        prev = curr\n",
    "    transition = transition_counts / priori_counts\n",
    "    transition[-1, 0] = 0  # ignore transition from END to START\n",
    "    # print(transition)\n",
    "    return transition\n",
    "\n",
    "def viterbi(transition, emission, all_unique_tokens, input_data):\n",
    "    # initial step\n",
    "    pi = [np.zeros([transition.shape[0], 1])]\n",
    "    pi[0][0] = 1\n",
    "    parents = []\n",
    "    j = 0\n",
    "    predicted_results = []\n",
    "    \n",
    "    for x in input_data:\n",
    "        if x != '':  # propagate with viterbi\n",
    "            b = get_emission_from_token(x, all_unique_tokens, emission)\n",
    "            res = np.matmul(pi[j], np.transpose(b)) * transition\n",
    "            # print(res.max(axis=0))\n",
    "            pi.append(np.reshape(res.max(axis=0), (-1, 1)))\n",
    "            parents.append(get_parents(res))\n",
    "            # parents.append(np.argmax(res, axis = 0))\n",
    "            j += 1\n",
    "        else:  # final step\n",
    "            # print(\"new sentence\")\n",
    "            res = pi[j] * transition[:,-1:]\n",
    "            output = get_parents(res)\n",
    "            # debug = [pi[j][output[0]]]\n",
    "\n",
    "            # output, trace back for the sequence\n",
    "            while j > 1:  # trace until second (first is START)\n",
    "                j -= 1\n",
    "                # debug.insert(0, pi[j][output[0]])\n",
    "                output.insert(0, parents[j][output[0]])\n",
    "            for i in output:\n",
    "                predicted_results.append(labels_list[i])\n",
    "            # for i in range(len(output)):\n",
    "            #     predicted_results.append(labels_list[output[i]] + ' ' + str(debug[i]))\n",
    "            predicted_results.append('')\n",
    "\n",
    "            # reset\n",
    "            pi = [np.zeros([N+2, 1])]\n",
    "            pi[0][0] = 1\n",
    "            parents = []\n",
    "            j = 0\n",
    "            # break\n",
    "\n",
    "    return predicted_results\n",
    "\n",
    "def viterbi_log(transition, emission, all_unique_tokens, input_data):\n",
    "    # initial step\n",
    "    log_transition = np.log(transition)\n",
    "    pi = [np.log(np.zeros([transition.shape[0], 1]))]\n",
    "    pi[0][0] = 0\n",
    "    parents = []\n",
    "    j = 0\n",
    "    predicted_results = []\n",
    "    \n",
    "    for x in input_data:\n",
    "        if x != '':  # propagate with viterbi\n",
    "            b = np.log(get_emission_from_token(x, all_unique_tokens, emission))\n",
    "            res = np.matmul(pi[j], np.transpose(np.ones(b.shape))) + np.matmul(np.ones(b.shape), np.transpose(b)) + log_transition\n",
    "            # print(res.max(axis=0))\n",
    "            # print(np.matmul(pi[j], np.transpose(np.zeros(b.shape))))\n",
    "            pi.append(np.reshape(res.max(axis=0), (-1, 1)))\n",
    "            parents.append(get_parents(res))\n",
    "            # parents.append(np.argmax(res, axis = 0))\n",
    "            j += 1\n",
    "        else:  # final step\n",
    "            # print(\"new sentence\")\n",
    "            res = pi[j] + log_transition[:,-1:]\n",
    "            output = get_parents(res)\n",
    "            # debug = [pi[j][output[0]]]\n",
    "\n",
    "            # output, trace back for the sequence\n",
    "            while j > 1:  # trace until second (first is START)\n",
    "                j -= 1\n",
    "                # debug.insert(0, pi[j][output[0]])\n",
    "                output.insert(0, parents[j][output[0]])\n",
    "            for i in output:\n",
    "                predicted_results.append(labels_list[i])\n",
    "            # for i in range(len(output)):\n",
    "            #     predicted_results.append(labels_list[output[i]] + ' ' + str(debug[i]))\n",
    "            predicted_results.append('')\n",
    "\n",
    "            # reset\n",
    "            pi = [np.log(np.zeros([transition.shape[0], 1]))]\n",
    "            pi[0][0] = 0\n",
    "            parents = []\n",
    "            j = 0\n",
    "            # break\n",
    "\n",
    "    return predicted_results\n",
    "\n",
    "def write_viterbi_output_to_file(language):\n",
    "    if language == \"ES\":\n",
    "        train_data = read_training_data(es_train_path)\n",
    "        train_data_w_end = read_training_data_w_end(es_train_path)\n",
    "        test_data = read_dev_in_data(es_dev_in_path)\n",
    "        output_path = es_dev_p2_out_path\n",
    "\n",
    "    elif language == \"RU\":\n",
    "        train_data = read_training_data(ru_train_path)\n",
    "        train_data_w_end = read_training_data_w_end(ru_train_path)\n",
    "        test_data = read_dev_in_data(ru_dev_in_path)\n",
    "        output_path = ru_dev_p2_out_path\n",
    "\n",
    "    # Conduct training/supervised learning (M-Step)\n",
    "    all_unique_tokens = get_all_unique_tokens(train_data)\n",
    "    emission_parameters = calculate_emission_parameters(train_data, all_unique_tokens)\n",
    "    transition_parameters = calculate_transition_parameters(train_data_w_end)\n",
    "\n",
    "    # Execute testing/decoding with Viterbi Algorithm (E-Step)\n",
    "    # predicted_results = viterbi(transition_parameters, emission_parameters, all_unique_tokens, test_data)\n",
    "    predicted_results = viterbi_log(transition_parameters, emission_parameters, all_unique_tokens, test_data)\n",
    "    with open(output_path, \"w+\", encoding=\"utf-8\") as file:\n",
    "        for i in range(len(test_data)):\n",
    "            if test_data[i] and predicted_results[i]:\n",
    "                file.write(\"{} {}\\n\".format(test_data[i], predicted_results[i]))\n",
    "            else:\n",
    "                file.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KEVINM~1\\AppData\\Local\\Temp/ipykernel_13968/161428581.py:78: RuntimeWarning: divide by zero encountered in log\n",
      "  log_transition = np.log(transition)\n",
      "C:\\Users\\KEVINM~1\\AppData\\Local\\Temp/ipykernel_13968/161428581.py:79: RuntimeWarning: divide by zero encountered in log\n",
      "  pi = [np.log(np.zeros([transition.shape[0], 1]))]\n",
      "C:\\Users\\KEVINM~1\\AppData\\Local\\Temp/ipykernel_13968/161428581.py:87: RuntimeWarning: divide by zero encountered in log\n",
      "  b = np.log(get_emission_from_token(x, all_unique_tokens, emission))\n",
      "C:\\Users\\KEVINM~1\\AppData\\Local\\Temp/ipykernel_13968/161428581.py:113: RuntimeWarning: divide by zero encountered in log\n",
      "  pi = [np.log(np.zeros([transition.shape[0], 1]))]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "for language in [\"ES\", \"RU\"]:\n",
    "    write_viterbi_output_to_file(language)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3 Code starting from here\n",
    "\n",
    "\n",
    "def viterbi_best_five(transition, emission, all_unique_tokens, input_data):\n",
    "    # initial step\n",
    "    pi = [np.zeros([transition.shape[0], 5])]\n",
    "    # print(pi)\n",
    "    # print(pi[0][:,0][np.newaxis].transpose())\n",
    "    for i in range(5):\n",
    "        pi[0][0,i] = 1\n",
    "    # print(pi)\n",
    "    parents = []\n",
    "    j = 0\n",
    "    predicted_results = [[],[],[],[],[]]\n",
    "    for x in input_data:\n",
    "        if x != '':  # propagate with viterbi\n",
    "            b = get_emission_from_token(x, all_unique_tokens, emission)\n",
    "\n",
    "            #get top 5\n",
    "            res_top=[]\n",
    "            for i in range(5):\n",
    "                res = np.matmul(pi[j][:,i][np.newaxis].transpose(), np.transpose(b)) * transition\n",
    "                res_top.append(res)\n",
    "            \n",
    "            res_final = np.concatenate((res_top[0],res_top[1],res_top[2],res_top[3],res_top[4]),axis=0)\n",
    "            #START\n",
    "            if j==0:\n",
    "                res_index_sorted=(-res_final).argsort(axis=0, kind=\"stable\")\n",
    "\n",
    "                #sort the res matrix \n",
    "                res_final=np.take_along_axis(res_final, res_index_sorted, axis=0)\n",
    "                # print(res_final[0:6])\n",
    "\n",
    "                # print(res_final[0:5,:].transpose())\n",
    "                pi.append(res_final[0:5].transpose())\n",
    "                # print(pi)\n",
    "                #change to top 5\n",
    "                parent_candidates=res_index_sorted[0:5]%transition.shape[0]\n",
    "                parents.append(parent_candidates)\n",
    "            else:\n",
    "                start_indices=list(range(0, res_final.shape[0], transition.shape[0]))\n",
    "                end_indices=[x+transition.shape[0]-1 for x in start_indices]\n",
    "                res_final=np.delete(res_final, start_indices + end_indices , axis=0)\n",
    "                res_index_sorted=(-res_final).argsort(axis=0, kind=\"stable\")\n",
    "\n",
    "\n",
    "                #sort the res matrix \n",
    "                res_final=np.take_along_axis(res_final, res_index_sorted, axis=0)\n",
    "                # print(res_final[0:6])\n",
    "\n",
    "                # print(res_final[0:5,:].transpose())\n",
    "                pi.append(res_final[0:5].transpose())\n",
    "                # print(pi)\n",
    "                #change to top 5\n",
    "                parent_candidates=res_index_sorted[0:5]%(transition.shape[0]-2)+1\n",
    "                parents.append(parent_candidates)\n",
    "            # parents.append(np.argmax(res, axis = 0))\n",
    "            j += 1\n",
    "        else:  # final step\n",
    "\n",
    "            #get top 5\n",
    "            res_top=[]\n",
    "            for i in range(5):\n",
    "                res = pi[j][:,i][np.newaxis].transpose() * transition[:,-1:]\n",
    "                res_top.append(res)\n",
    "            \n",
    "            res_final = np.concatenate((res_top[0],res_top[1],res_top[2],res_top[3],res_top[4]),axis=0)\n",
    "            \n",
    "            start_indices=list(range(0, res_final.shape[0], transition.shape[0]))\n",
    "            end_indices=[x+transition.shape[0]-1 for x in start_indices]\n",
    "            res_final=np.delete(res_final, start_indices + end_indices , axis=0)\n",
    "            res_index_sorted=(-res_final).argsort(axis=0, kind=\"stable\")\n",
    "\n",
    "\n",
    "            #sort the res matrix \n",
    "            res_final=np.take_along_axis(res_final, res_index_sorted, axis=0)\n",
    "            # print(res_final[0:6])\n",
    "\n",
    "            # print(res_final[0:5,:].transpose())\n",
    "            pi.append(res_final[0:5].transpose())\n",
    "            # print(pi)\n",
    "            #change to top 5\n",
    "            parent_candidates=res_index_sorted[0:5]%(transition.shape[0]-2)+1\n",
    "            # parents.append(parent_candidates)\n",
    "\n",
    "            # res = pi[j] * transition[:,-1:]\n",
    "\n",
    "            output = [parent_candidates]\n",
    "            # print(output)\n",
    "            # debug = [pi[j][output[0]]]\n",
    "            # output, trace back for the sequence\n",
    "            while j > 1:  # trace until second (first is START)\n",
    "                j -= 1\n",
    "                # debug.insert(0, pi[j][output[0]])\n",
    "                # output.insert(0, parents[j][output[0]])\n",
    "                all_predecessor_candidates=[]\n",
    "                all_predecessor_scores=[]\n",
    "\n",
    "                for index,output_candidate in enumerate( output[0]):\n",
    "\n",
    "                    #get top 5\n",
    "                    res_top=[]\n",
    "                    for i in range(5):\n",
    "                        # print(\"j\",j,len(pi))\n",
    "                        res = pi[j][:,i][np.newaxis].transpose() * transition\n",
    "                        res_top.append(res)\n",
    "                    res_final = np.concatenate((res_top[0],res_top[1],res_top[2],res_top[3],res_top[4]),axis=0)\n",
    "                    # res_final=np.delete(res_final, start_indices + end_indices , axis=0)\n",
    "                    # res_index_sorted=(-res_final).argsort(axis=0, kind=\"stable\")\n",
    "                    # #sort the res matrix \n",
    "                    # res_final=np.take_along_axis(res_final, res_index_sorted, axis=0)\n",
    "                    # print(\"res final\")\n",
    "                    # print(res_final)\n",
    "                    # print(\"res final end\" )\n",
    "\n",
    "\n",
    "                    predecessor_candidates=parents[j][:,output_candidate]\n",
    "                    predecessor_scores=[]\n",
    "                    # print(output_candidate)\n",
    "                    # print(predecessor_candidates.shape,j)\n",
    "                    # print(parents[j].shape)\n",
    "                    # print(\"predecessors\")\n",
    "                    # print(predecessor_candidates)\n",
    "                    candidate_count=np.zeros([transition.shape[0]])\n",
    "                    for cand in predecessor_candidates:\n",
    "                        candidate_id=candidate_count[cand]\n",
    "                        # print(candidate_id,transition.shape[1],cand)\n",
    "                        predecessor_scores.append(res_final[int(candidate_id*transition.shape[1]+cand)])\n",
    "                        candidate_count[cand]+=1\n",
    "                    all_predecessor_candidates.append(np.array(predecessor_candidates))\n",
    "                    all_predecessor_scores.append(np.array(predecessor_scores))\n",
    "                predec_score_array=np.concatenate((all_predecessor_scores[0],all_predecessor_scores[1],all_predecessor_scores[2],all_predecessor_scores[3],all_predecessor_scores[4]),axis=0)\n",
    "\n",
    "                predec_cand_array=np.concatenate((all_predecessor_candidates[0],all_predecessor_candidates[1],all_predecessor_candidates[2],all_predecessor_candidates[3],all_predecessor_candidates[4]),axis=0)\n",
    "                sorting_array=np.argsort(predec_score_array,kind=\"stable\")\n",
    "\n",
    "                prefec_cand_array = np.take_along_axis(predec_cand_array, sorting_array, axis=0)\n",
    "                # predec_cand_array=predec_cand_array[sorting_array]\n",
    "\n",
    "                predec_cand_array=predec_cand_array[0:5]\n",
    "                # print(predec_cand_array.shape,j)\n",
    "                output.insert(0,predec_cand_array)\n",
    "            # print(output)\n",
    "            for best_order in range(5):\n",
    "                for i in range(len(output)):\n",
    "                    predicted_results[best_order].append(labels_list[int(output[i][best_order])%transition.shape[1]])\n",
    "                # for i in range(len(output)):\n",
    "                #     predicted_results.append(labels_list[output[i]] + ' ' + str(debug[i]))\n",
    "                predicted_results[best_order].append('')\n",
    "\n",
    "            # reset\n",
    "            pi = [np.zeros([transition.shape[0], 5])]\n",
    "            # print(pi)\n",
    "            # print(pi[0][:,0][np.newaxis].transpose())\n",
    "            for i in range(5):\n",
    "                pi[0][0,i] = 1\n",
    "            parents = []\n",
    "            j = 0\n",
    "            # break\n",
    "\n",
    "    return predicted_results\n",
    "\n",
    "def write_viterbi_output_to_file_best_five(language):\n",
    "    if language == \"ES\":\n",
    "        train_data = read_training_data(es_train_path)\n",
    "        train_data_w_end = read_training_data_w_end(es_train_path)\n",
    "        test_data = read_dev_in_data(es_dev_in_path)\n",
    "        output_path = es_dev_p3_out_path\n",
    "\n",
    "    elif language == \"RU\":\n",
    "        train_data = read_training_data(ru_train_path)\n",
    "        train_data_w_end = read_training_data_w_end(ru_train_path)\n",
    "        test_data = read_dev_in_data(ru_dev_in_path)\n",
    "        output_path = ru_dev_p3_out_path\n",
    "\n",
    "    # Conduct training/supervised learning (M-Step)\n",
    "    all_unique_tokens = get_all_unique_tokens(train_data)\n",
    "    emission_parameters = calculate_emission_parameters(train_data, all_unique_tokens)\n",
    "    transition_parameters = calculate_transition_parameters(train_data_w_end)\n",
    "\n",
    "    # Execute testing/decoding with Viterbi Algorithm (E-Step)\n",
    "    predicted_results = viterbi_best_five(transition_parameters, emission_parameters, all_unique_tokens, test_data)\n",
    "    with open(output_path, \"w+\", encoding=\"utf-8\") as file:\n",
    "        for i in range(len(test_data)):\n",
    "            if i<len(test_data) and i< len(predicted_results[0]) and test_data[i] and predicted_results[0][i]:\n",
    "                file.write(\"{} {} {} {} {} {}\\n\".format(test_data[i], predicted_results[0][i], predicted_results[1][i], predicted_results[2][i], predicted_results[3][i], predicted_results[4][i]))\n",
    "            else:\n",
    "                file.write(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "for language in [\"ES\", \"RU\"]:\n",
    "    write_viterbi_output_to_file_best_five(language)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
