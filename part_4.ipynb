{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUTD 2021 50.007 Machine Learning HMM Project Part 4\n",
    "\n",
    "> Group 4:\n",
    "> - Ma Yuchen (1004519)\n",
    "> - Chung Wah Kit (1004103)\n",
    "> - James Raphael Tiovalen (1004555)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Workspace Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Setup and install dependencies\n",
    "# !pip3 install numpy\n",
    "# !pip3 install torch\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(0)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Enable floating-point underflow warnings and disable divide-by-zero warnings\n",
    "np.seterr(under=\"warn\", divide=\"ignore\")\n",
    "\n",
    "# Set OS-independent paths, relative to current directory\n",
    "es_train_path = os.path.join(\"data\", \"ES\", \"train\")\n",
    "es_dev_in_path = os.path.join(\"data\", \"ES\", \"dev.in\")\n",
    "es_dev_out_path = os.path.join(\"data\", \"ES\", \"dev.out\")\n",
    "es_dev_p1_out_path = os.path.join(\"data\", \"ES\", \"dev.p1.out\")\n",
    "es_dev_p2_out_path = os.path.join(\"data\", \"ES\", \"dev.p2.out\")\n",
    "es_dev_p3_out_path = os.path.join(\"data\", \"ES\", \"dev.p3.out\")\n",
    "es_dev_p4_out_path = os.path.join(\"data\", \"ES\", \"dev.p4.out\")\n",
    "es_test_in_path = os.path.join(\"data\", \"ES\", \"test.in\")\n",
    "es_test_out_path = os.path.join(\"data\", \"ES\", \"test.p4.out\")\n",
    "ru_train_path = os.path.join(\"data\", \"RU\", \"train\")\n",
    "ru_dev_in_path = os.path.join(\"data\", \"RU\", \"dev.in\")\n",
    "ru_dev_out_path = os.path.join(\"data\", \"RU\", \"dev.out\")\n",
    "ru_dev_p1_out_path = os.path.join(\"data\", \"RU\", \"dev.p1.out\")\n",
    "ru_dev_p2_out_path = os.path.join(\"data\", \"RU\", \"dev.p2.out\")\n",
    "ru_dev_p3_out_path = os.path.join(\"data\", \"RU\", \"dev.p3.out\")\n",
    "ru_dev_p4_out_path = os.path.join(\"data\", \"RU\", \"dev.p4.out\")\n",
    "ru_test_in_path = os.path.join(\"data\", \"RU\", \"test.in\")\n",
    "ru_test_out_path = os.path.join(\"data\", \"RU\", \"test.p4.out\")\n",
    "\n",
    "\n",
    "# Define constant variables\n",
    "N = 7\n",
    "O, BPOS, IPOS, BNEU, INEU, BNEG, INEG = 0, 1, 2, 3, 4, 5, 6\n",
    "label_to_id = {\"O\": O,\n",
    "          \"B-positive\": BPOS,\n",
    "          \"I-positive\": IPOS,\n",
    "          \"B-neutral\": BNEU,\n",
    "          \"I-neutral\": INEU,\n",
    "          \"B-negative\": BNEG,\n",
    "          \"I-negative\": INEG,}\n",
    "id_to_label = [\"O\", \"B-positive\", \"I-positive\", \"B-neutral\", \"I-neutral\", \"B-negative\", \"I-negative\"]\n",
    "\n",
    "# Initialise a random number generator with a fixed seed for reproducible results and deterministic behavior\n",
    "rng = np.random.default_rng(1004519 + 1004103 + 1004555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dev.in data\n",
    "def read_dev_in_data(filepath):\n",
    "    results = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        sentence = []\n",
    "        for line in lines:\n",
    "            if line.strip() != '':\n",
    "                sentence.append(line.strip())  # Add zero if we meet an unknown token\n",
    "            else:\n",
    "                results.append(sentence.copy())\n",
    "                sentence = []\n",
    "    return results\n",
    "\n",
    "# Read training data\n",
    "def read_training_data(filepath):\n",
    "    results = []\n",
    "    vocab = {'': 0}\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for line in lines:\n",
    "            if len(line.strip().rsplit(\" \", 1)) == 2:\n",
    "                token, label = line.strip().rsplit(\" \", 1)\n",
    "                token = token.lower()\n",
    "                if token not in vocab:\n",
    "                    vocab[token] = len(vocab)\n",
    "                tokens.append(vocab[token])\n",
    "                labels.append(label_to_id[label])\n",
    "            else:\n",
    "                results.append((tokens.copy(), labels.copy()))\n",
    "                tokens = []\n",
    "                labels = []\n",
    "    return results, vocab\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix.get(w.lower(), 0) for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, bidirectional=False)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG = 'es'\n",
    "\n",
    "if LANG == 'es':\n",
    "    # Load training data\n",
    "    training_data, word_to_id = read_training_data(es_train_path)\n",
    "    # print(word_to_id)\n",
    "elif LANG == 'ru':\n",
    "    # Load training data\n",
    "    training_data, word_to_id = read_training_data(ru_train_path)\n",
    "    # print(word_to_id)\n",
    "\n",
    "EMBEDDING_DIM = 16\n",
    "HIDDEN_DIM = 16\n",
    "NUM_LAYERS = 4\n",
    "\n",
    "# dev.p4.out statistics for current best test.p4.out files:\n",
    "# ES: 25 epochs\n",
    "# Loss: 0.03923250734806061\n",
    "# Entity F: 0.6081\n",
    "# Sentiment F: 0.4955\n",
    "# RU: 45 epochs\n",
    "# Loss: 0.036458928138017654\n",
    "# Entity F: 0.5487\n",
    "# Sentiment F: 0.4043"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMTagger(\n",
       "  (word_embeddings): Embedding(4805, 16)\n",
       "  (lstm): LSTM(16, 16, num_layers=4)\n",
       "  (hidden2tag): Linear(in_features=16, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, len(word_to_id), len(label_to_id))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# with torch.no_grad():\n",
    "#     inputs = torch.tensor(training_data[0][0], dtype=torch.long)\n",
    "#     tag_scores = model(inputs)\n",
    "#     print(tag_scores)\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Loss: 0.139827162027359\n",
      "Epoch 1\n",
      "Loss: 0.13825173676013947\n",
      "Epoch 2\n",
      "Loss: 0.13662147521972656\n",
      "Epoch 3\n",
      "Loss: 0.13492418825626373\n",
      "Epoch 4\n",
      "Loss: 0.1340443640947342\n",
      "Epoch 5\n",
      "Loss: 0.13356293737888336\n",
      "Epoch 6\n",
      "Loss: 0.1332509070634842\n",
      "Epoch 7\n",
      "Loss: 0.13304616510868073\n",
      "Epoch 8\n",
      "Loss: 0.13294288516044617\n",
      "Epoch 9\n",
      "Loss: 0.1329396814107895\n",
      "Epoch 10\n",
      "Loss: 0.13298451900482178\n",
      "Epoch 11\n",
      "Loss: 0.13275621831417084\n",
      "Epoch 12\n",
      "Loss: 0.1306191384792328\n",
      "Epoch 13\n",
      "Loss: 0.1243438720703125\n",
      "Epoch 14\n",
      "Loss: 0.1141945868730545\n",
      "Epoch 15\n",
      "Loss: 0.08683393895626068\n",
      "Epoch 16\n",
      "Loss: 0.061229005455970764\n",
      "Epoch 17\n",
      "Loss: 0.049809232354164124\n",
      "Epoch 18\n",
      "Loss: 0.042533375322818756\n",
      "Epoch 19\n",
      "Loss: 0.06623648852109909\n",
      "Epoch 20\n",
      "Loss: 0.1282656043767929\n",
      "Epoch 21\n",
      "Loss: 0.12623585760593414\n",
      "Epoch 22\n",
      "Loss: 0.11663979291915894\n",
      "Epoch 23\n",
      "Loss: 0.07129455357789993\n",
      "Epoch 24\n",
      "Loss: 0.030190762132406235\n",
      "Epoch 25\n",
      "Loss: 0.022595878690481186\n",
      "Training done!\n"
     ]
    }
   ],
   "source": [
    "# s = 0\n",
    "for epoch in range(26):\n",
    "    print(\"Epoch\", epoch)\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance.\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into tensors of word indices.\n",
    "        sentence_in = torch.tensor(sentence, dtype=torch.long).type(torch.cuda.LongTensor)\n",
    "        targets = torch.tensor(tags, dtype=torch.long).type(torch.cuda.LongTensor)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Loss:\", loss.item())\n",
    "    # if s == 0 and loss.item() < 0.01:\n",
    "    #     s = 1\n",
    "    # elif s == 1 and loss.item() > 0.05:\n",
    "    #     break\n",
    "\n",
    "# See what the scores are after training\n",
    "# with torch.no_grad():\n",
    "#     inputs = prepare_sequence(test_data[0], word_to_id).type(torch.cuda.LongTensor)\n",
    "#     tag_scores = model(inputs)\n",
    "#     print(tag_scores)\n",
    "\n",
    "print(\"Training done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running on dev.in\n",
    "if LANG == 'es':\n",
    "    test_data = read_dev_in_data(es_dev_in_path)\n",
    "    path = es_dev_p4_out_path\n",
    "elif LANG == 'ru':\n",
    "    test_data = read_dev_in_data(ru_dev_in_path)\n",
    "    path = ru_dev_p4_out_path\n",
    "\n",
    "with torch.no_grad():\n",
    "    with open(path, \"w+\", encoding=\"utf-8\") as file:\n",
    "        for sentence in test_data:\n",
    "            inputs = prepare_sequence(sentence, word_to_id).type(torch.cuda.LongTensor)\n",
    "            tag_scores = np.array(model(inputs).cpu())\n",
    "            pred_labels = np.argmax(tag_scores, axis=1)\n",
    "            for i in range(len(sentence)):\n",
    "                file.write(\"{} {}\\n\".format(sentence[i], id_to_label[pred_labels[i]]))\n",
    "            file.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running on test.in\n",
    "if LANG == 'es':\n",
    "    test_data = read_dev_in_data(es_test_in_path)\n",
    "    path = es_test_out_path\n",
    "elif LANG == 'ru':\n",
    "    test_data = read_dev_in_data(ru_test_in_path)\n",
    "    path = ru_test_out_path\n",
    "\n",
    "with torch.no_grad():\n",
    "    with open(path, \"w+\", encoding=\"utf-8\") as file:\n",
    "        for sentence in test_data:\n",
    "            inputs = prepare_sequence(sentence, word_to_id).type(torch.cuda.LongTensor)\n",
    "            tag_scores = np.array(model(inputs).cpu())\n",
    "            pred_labels = np.argmax(tag_scores, axis=1)\n",
    "            for i in range(len(sentence)):\n",
    "                file.write(\"{} {}\\n\".format(sentence[i], id_to_label[pred_labels[i]]))\n",
    "            file.write(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a542ca580817fc4dd55327026e074e2fa0cd470fc5dee9350c2d8b13822db8d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
