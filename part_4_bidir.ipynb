{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and install dependencies\n",
    "# !pip3 install numpy\n",
    "# !pip3 install torch\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# from gensim.models import Word2Vec\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Enable floating-point underflow warning\n",
    "np.seterr(under=\"warn\")\n",
    "\n",
    "# Set OS-independent paths, relative to current directory\n",
    "es_train_path = os.path.join(\"data\", \"ES\", \"train\")\n",
    "es_dev_in_path = os.path.join(\"data\", \"ES\", \"dev.in\")\n",
    "es_dev_out_path = os.path.join(\"data\", \"ES\", \"dev.out\")\n",
    "es_dev_p1_out_path = os.path.join(\"data\", \"ES\", \"dev.p1.out\")\n",
    "es_dev_p2_out_path = os.path.join(\"data\", \"ES\", \"dev.p2.out\")\n",
    "es_dev_p3_out_path = os.path.join(\"data\", \"ES\", \"dev.p3.out\")\n",
    "es_dev_p4_out_path = os.path.join(\"data\", \"ES\", \"dev.p4.out\")\n",
    "es_test_in_path = os.path.join(\"data\", \"ES-test\", \"test.in\")\n",
    "es_test_out_path = os.path.join(\"data\", \"ES-test\", \"test.out\")\n",
    "ru_train_path = os.path.join(\"data\", \"RU\", \"train\")\n",
    "ru_dev_in_path = os.path.join(\"data\", \"RU\", \"dev.in\")\n",
    "ru_dev_out_path = os.path.join(\"data\", \"RU\", \"dev.out\")\n",
    "ru_dev_p1_out_path = os.path.join(\"data\", \"RU\", \"dev.p1.out\")\n",
    "ru_dev_p2_out_path = os.path.join(\"data\", \"RU\", \"dev.p2.out\")\n",
    "ru_dev_p3_out_path = os.path.join(\"data\", \"RU\", \"dev.p3.out\")\n",
    "ru_dev_p4_out_path = os.path.join(\"data\", \"RU\", \"dev.p4.out\")\n",
    "ru_test_in_path = os.path.join(\"data\", \"RU-test\", \"test.in\")\n",
    "ru_test_out_path = os.path.join(\"data\", \"RU-test\", \"test.out\")\n",
    "\n",
    "\n",
    "# Define constant variables\n",
    "N = 7\n",
    "O, BPOS, IPOS, BNEU, INEU, BNEG, INEG = 0, 1, 2, 3, 4, 5, 6\n",
    "label_to_id = {\"O\": O,\n",
    "          \"B-positive\": BPOS,\n",
    "          \"I-positive\": IPOS,\n",
    "          \"B-neutral\": BNEU,\n",
    "          \"I-neutral\": INEU,\n",
    "          \"B-negative\": BNEG,\n",
    "          \"I-negative\": INEG,}\n",
    "id_to_label = [\"O\", \"O\", \"B-positive\", \"I-positive\", \"B-neutral\", \"I-neutral\", \"B-negative\", \"I-negative\"]\n",
    "\n",
    "# Initialise a random number generator with a fixed seed for reproducible results and deterministic behavior\n",
    "# rng = np.random.default_rng(1004519 + 1004103 + 1004555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dev.in data\n",
    "def read_dev_in_data(filepath):\n",
    "    results = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        sentence = []\n",
    "        for line in lines:\n",
    "            if line.strip() != '':\n",
    "                sentence.append(line.strip())  # add zero if meet unkown token\n",
    "            else:\n",
    "                results.append(sentence.copy())\n",
    "                sentence = []\n",
    "    return results\n",
    "\n",
    "# Read training data\n",
    "def read_training_data(filepath):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for line in lines:\n",
    "            if len(line.strip().rsplit(\" \", 1)) == 2:\n",
    "                token, label = line.strip().rsplit(\" \", 1)\n",
    "                tokens.append(token.lower())\n",
    "                labels.append(label)\n",
    "            else:\n",
    "                X.append(tokens.copy())\n",
    "                Y.append(labels.copy())\n",
    "                tokens = []\n",
    "                labels = []\n",
    "    return X, Y\n",
    "\n",
    "# def prepare_sequence(seq, to_ix):\n",
    "#     idxs = [to_ix.get(w.lower(), 0) for w in seq]\n",
    "#     return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Raw data point ** \n",
      " ---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "X:  ['disfrutemos', 'de', 'una', 'buenísima', 'calidad', 'en', 'el', 'producto', 'y', 'una', 'inmejorable', 'relación', 'calidad', 'precio', '.'] \n",
      "\n",
      "Y:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-positive', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] \n",
      "\n",
      "\n",
      "** Encoded data point ** \n",
      " ---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "X:  [1904, 3, 18, 760, 27, 8, 7, 228, 4, 18, 229, 75, 27, 35, 2] \n",
      "\n",
      "Y:  [1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1] \n",
      "\n",
      "0 sentences have disparate input-output lengths.\n"
     ]
    }
   ],
   "source": [
    "X, Y = read_training_data(es_train_path)\n",
    "\n",
    "# encode X\n",
    "word_tokenizer = Tokenizer()              # instantiate tokeniser\n",
    "word_tokenizer.fit_on_texts(X)            # fit tokeniser on data\n",
    "# use the tokeniser to encode input sequence\n",
    "X_encoded = word_tokenizer.texts_to_sequences(X)\n",
    "\n",
    "\n",
    "# encode Y\n",
    "tag_tokenizer = Tokenizer()\n",
    "tag_tokenizer.fit_on_texts(Y)\n",
    "Y_encoded = tag_tokenizer.texts_to_sequences(Y)\n",
    "\n",
    "# look at first encoded data point\n",
    "print(\"** Raw data point **\", \"\\n\", \"-\"*100, \"\\n\")\n",
    "print('X: ', X[0], '\\n')\n",
    "print('Y: ', Y[0], '\\n')\n",
    "print()\n",
    "print(\"** Encoded data point **\", \"\\n\", \"-\"*100, \"\\n\")\n",
    "print('X: ', X_encoded[0], '\\n')\n",
    "print('Y: ', Y_encoded[0], '\\n')\n",
    "\n",
    "# make sure that each sequence of input and output is same length\n",
    "different_length = [1 if len(input) != len(output) else 0 for input, output in zip(X_encoded, Y_encoded)]\n",
    "print(\"{} sentences have disparate input-output lengths.\".format(sum(different_length)))\n",
    "#187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of longest sentence: 163\n"
     ]
    }
   ],
   "source": [
    "# check length of longest sentence\n",
    "lengths = [len(seq) for seq in X_encoded]\n",
    "print(\"Length of longest sentence: {}\".format(max(lengths)))\n",
    "\n",
    "# sns.boxplot(lengths)\n",
    "# plt.show()\n",
    "\n",
    "EMBEDDING_SIZE = 300\n",
    "MAX_SEQ_LENGTH = 200  # sequences greater than 100 in length will be truncated\n",
    "VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "X_padded = pad_sequences(X_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "Y_padded = pad_sequences(Y_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "\n",
    "# print(X_padded[0], \"\\n\")\n",
    "# print(Y_padded[0])\n",
    "\n",
    "X, Y = X_padded, Y_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA\n",
      "Shape of input sequences: (1858, 200)\n",
      "Shape of output sequences: (1858, 200, 8)\n",
      "--------------------------------------------------\n",
      "VALIDATION DATA\n",
      "Shape of input sequences: (207, 200)\n",
      "Shape of output sequences: (207, 200, 8)\n",
      "--------------------------------------------------\n",
      "TESTING DATA\n",
      "Shape of input sequences: (310, 100)\n",
      "Shape of output sequences: (310, 100, 8)\n"
     ]
    }
   ],
   "source": [
    "Y = to_categorical(Y)\n",
    "# print(Y.shape)\n",
    "NUM_CLASSES = Y.shape[2]\n",
    "\n",
    "# split entire data into training and testing sets\n",
    "# TEST_SIZE = 0.15\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=TEST_SIZE, random_state=4)\n",
    "\n",
    "# split training data into training and validation sets\n",
    "VALID_SIZE = 0.1\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=VALID_SIZE, random_state=4)\n",
    "\n",
    "# print number of samples in each set\n",
    "print(\"TRAINING DATA\")\n",
    "print('Shape of input sequences: {}'.format(X_train.shape))\n",
    "print('Shape of output sequences: {}'.format(Y_train.shape))\n",
    "print(\"-\"*50)\n",
    "print(\"VALIDATION DATA\")\n",
    "print('Shape of input sequences: {}'.format(X_validation.shape))\n",
    "print('Shape of output sequences: {}'.format(Y_validation.shape))\n",
    "print(\"-\"*50)\n",
    "print(\"TESTING DATA\")\n",
    "print('Shape of input sequences: {}'.format(X_test.shape))\n",
    "print('Shape of output sequences: {}'.format(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 200, 300)          1441500   \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 200, 256)          439296    \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 200, 8)            2056      \n",
      "=================================================================\n",
      "Total params: 1,882,852\n",
      "Trainable params: 1,882,852\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bidirect_model = Sequential()\n",
    "bidirect_model.add(Embedding(input_dim     = VOCABULARY_SIZE,\n",
    "                             output_dim    = EMBEDDING_SIZE,\n",
    "                             input_length  = MAX_SEQ_LENGTH,\n",
    "                             trainable     = True\n",
    "))\n",
    "bidirect_model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "# bidirect_model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "bidirect_model.add(TimeDistributed(Dense(NUM_CLASSES, activation='softmax')))\n",
    "\n",
    "bidirect_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "bidirect_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "15/15 [==============================] - 28s 2s/step - loss: 0.0045 - acc: 0.9989 - val_loss: 0.0201 - val_acc: 0.9958\n",
      "Epoch 2/20\n",
      "15/15 [==============================] - 28s 2s/step - loss: 0.0043 - acc: 0.9989 - val_loss: 0.0216 - val_acc: 0.9957\n",
      "Epoch 3/20\n",
      "15/15 [==============================] - 28s 2s/step - loss: 0.0040 - acc: 0.9990 - val_loss: 0.0232 - val_acc: 0.9958\n",
      "Epoch 4/20\n",
      "15/15 [==============================] - 28s 2s/step - loss: 0.0038 - acc: 0.9991 - val_loss: 0.0237 - val_acc: 0.9958\n",
      "Epoch 5/20\n",
      "15/15 [==============================] - 28s 2s/step - loss: 0.0036 - acc: 0.9991 - val_loss: 0.0219 - val_acc: 0.9959\n",
      "Epoch 6/20\n",
      "15/15 [==============================] - 28s 2s/step - loss: 0.0034 - acc: 0.9991 - val_loss: 0.0240 - val_acc: 0.9954\n",
      "Epoch 7/20\n",
      "15/15 [==============================] - 28s 2s/step - loss: 0.0032 - acc: 0.9992 - val_loss: 0.0227 - val_acc: 0.9958\n",
      "Epoch 8/20\n",
      "15/15 [==============================] - 26s 2s/step - loss: 0.0030 - acc: 0.9993 - val_loss: 0.0237 - val_acc: 0.9958\n",
      "Epoch 9/20\n",
      "15/15 [==============================] - 26s 2s/step - loss: 0.0028 - acc: 0.9993 - val_loss: 0.0232 - val_acc: 0.9958\n",
      "Epoch 10/20\n",
      "15/15 [==============================] - 26s 2s/step - loss: 0.0026 - acc: 0.9994 - val_loss: 0.0249 - val_acc: 0.9957\n",
      "Epoch 11/20\n",
      "15/15 [==============================] - 26s 2s/step - loss: 0.0025 - acc: 0.9994 - val_loss: 0.0250 - val_acc: 0.9956\n",
      "Epoch 12/20\n",
      "15/15 [==============================] - 26s 2s/step - loss: 0.0023 - acc: 0.9994 - val_loss: 0.0245 - val_acc: 0.9958\n",
      "Epoch 13/20\n",
      "15/15 [==============================] - 26s 2s/step - loss: 0.0022 - acc: 0.9995 - val_loss: 0.0245 - val_acc: 0.9958\n",
      "Epoch 14/20\n",
      "15/15 [==============================] - 26s 2s/step - loss: 0.0022 - acc: 0.9995 - val_loss: 0.0239 - val_acc: 0.9955\n",
      "Epoch 15/20\n",
      "15/15 [==============================] - 26s 2s/step - loss: 0.0021 - acc: 0.9995 - val_loss: 0.0273 - val_acc: 0.9957\n",
      "Epoch 16/20\n",
      "15/15 [==============================] - 26s 2s/step - loss: 0.0019 - acc: 0.9996 - val_loss: 0.0262 - val_acc: 0.9958\n",
      "Epoch 17/20\n",
      "15/15 [==============================] - 26s 2s/step - loss: 0.0017 - acc: 0.9996 - val_loss: 0.0256 - val_acc: 0.9957\n",
      "Epoch 18/20\n",
      "15/15 [==============================] - 26s 2s/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.0271 - val_acc: 0.9957\n",
      "Epoch 19/20\n",
      "15/15 [==============================] - 26s 2s/step - loss: 0.0015 - acc: 0.9997 - val_loss: 0.0275 - val_acc: 0.9957\n",
      "Epoch 20/20\n",
      "15/15 [==============================] - 27s 2s/step - loss: 0.0014 - acc: 0.9997 - val_loss: 0.0277 - val_acc: 0.9957\n"
     ]
    }
   ],
   "source": [
    "bidirect_training = bidirect_model.fit(X_train, Y_train, batch_size=128, epochs=20, validation_data=(X_validation, Y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_raw = read_dev_in_data(es_dev_in_path)\n",
    "X_test_encoded = word_tokenizer.texts_to_sequences(X_test_raw)\n",
    "X_test_padded = pad_sequences(X_test_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "\n",
    "res = bidirect_model.apply(X_test_padded)\n",
    "index = np.argmax(res, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_label = [\"O\", \"O\", \"B-positive\", \"I-positive\", \"B-neutral\", \"I-neutral\", \"B-negative\", \"I-negative\"]\n",
    "with open(es_dev_p4_out_path, \"w+\", encoding=\"utf-8\") as file:\n",
    "    for i in range(len(X_test_raw)):\n",
    "        sentence = X_test_raw[i]\n",
    "        pred_labels = index[i][-len(sentence):]\n",
    "        for j in range(len(sentence)):\n",
    "            # print(\"{} {}\\n\".format(sentence[j], id_to_label[pred_labels[j]]))\n",
    "            file.write(\"{} {}\\n\".format(sentence[j], id_to_label[pred_labels[j]]))\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG = 'ru'\n",
    "\n",
    "if LANG == 'es':\n",
    "    # load training data:\n",
    "    training_data, word_to_id = read_training_data(es_train_path)\n",
    "    # print(word_to_id)\n",
    "elif LANG == 'ru':\n",
    "    # load training data:\n",
    "    training_data, word_to_id = read_training_data(ru_train_path)\n",
    "    # print(word_to_id)\n",
    "\n",
    "EMBEDDING_DIM = 16\n",
    "HIDDEN_DIM = 16\n",
    "NUM_LAYERS = 4\n",
    "\n",
    "# ES: 25 epochs\n",
    "# Loss: 0.03923250734806061\n",
    "# Entity F: 0.6081\n",
    "# Sentiment F: 0.4955\n",
    "# RU: 45 epochs\n",
    "# Loss: 0.036458928138017654\n",
    "# Entity F: 0.5487\n",
    "# Sentiment F: 0.4043"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMTagger(\n",
       "  (word_embeddings): Embedding(7480, 16)\n",
       "  (lstm): LSTM(16, 16, num_layers=4)\n",
       "  (hidden2tag): Linear(in_features=16, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, len(word_to_id), len(label_to_id))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# with torch.no_grad():\n",
    "#     inputs = torch.tensor(training_data[0][0], dtype=torch.long)\n",
    "#     tag_scores = model(inputs)\n",
    "#     print(tag_scores)\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0\n",
      "0.026569059118628502\n",
      "epoch  1\n",
      "0.03420393541455269\n",
      "epoch  2\n",
      "0.031586337834596634\n",
      "epoch  3\n",
      "0.030467532575130463\n",
      "epoch  4\n",
      "0.027004273608326912\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# s = 0\n",
    "for epoch in range(5):  # again, normally you would NOT do 300 epochs, it is toy data   \n",
    "    print(\"epoch \", epoch)\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = torch.tensor(sentence, dtype=torch.long).type(torch.cuda.LongTensor)\n",
    "        targets = torch.tensor(tags, dtype=torch.long).type(torch.cuda.LongTensor)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss.item())\n",
    "    # if s == 0 and loss.item() < 0.01:\n",
    "    #     s = 1\n",
    "    # elif s == 1 and loss.item() > 0.05:\n",
    "    #     break\n",
    "\n",
    "# See what the scores are after training\n",
    "# with torch.no_grad():\n",
    "#     inputs = prepare_sequence(test_data[0], word_to_id).type(torch.cuda.LongTensor)\n",
    "#     tag_scores = model(inputs)\n",
    "\n",
    "#     print(tag_scores)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running on dev.in\n",
    "if LANG == 'es':\n",
    "    test_data = read_dev_in_data(es_dev_in_path)\n",
    "    path = es_dev_p4_out_path\n",
    "elif LANG == 'ru':\n",
    "    test_data = read_dev_in_data(ru_dev_in_path)\n",
    "    path = ru_dev_p4_out_path\n",
    "\n",
    "with torch.no_grad():\n",
    "    with open(path, \"w+\", encoding=\"utf-8\") as file:\n",
    "        for sentence in test_data:\n",
    "            inputs = prepare_sequence(sentence, word_to_id).type(torch.cuda.LongTensor)\n",
    "            tag_scores = np.array(model(inputs).cpu())\n",
    "            pred_labels = np.argmax(tag_scores, axis=1)\n",
    "            for i in range(len(sentence)):\n",
    "                file.write(\"{} {}\\n\".format(sentence[i], id_to_label[pred_labels[i]]))\n",
    "            file.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running on test.in\n",
    "if LANG == 'es':\n",
    "    test_data = read_dev_in_data(es_test_in_path)\n",
    "    path = es_test_out_path\n",
    "elif LANG == 'ru':\n",
    "    test_data = read_dev_in_data(ru_test_in_path)\n",
    "    path = ru_test_out_path\n",
    "\n",
    "with torch.no_grad():\n",
    "    with open(path, \"w+\", encoding=\"utf-8\") as file:\n",
    "        for sentence in test_data:\n",
    "            inputs = prepare_sequence(sentence, word_to_id).type(torch.cuda.LongTensor)\n",
    "            tag_scores = np.array(model(inputs).cpu())\n",
    "            pred_labels = np.argmax(tag_scores, axis=1)\n",
    "            for i in range(len(sentence)):\n",
    "                file.write(\"{} {}\\n\".format(sentence[i], id_to_label[pred_labels[i]]))\n",
    "            file.write(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a542ca580817fc4dd55327026e074e2fa0cd470fc5dee9350c2d8b13822db8d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
