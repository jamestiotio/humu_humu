{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Setup and install dependencies\n",
    "# !pip3 install numpy\n",
    "# !pip3 install torch\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(0)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Enable floating-point underflow warning\n",
    "np.seterr(under=\"warn\")\n",
    "\n",
    "# Set OS-independent paths, relative to current directory\n",
    "es_train_path = os.path.join(\"data\", \"ES\", \"train\")\n",
    "es_dev_in_path = os.path.join(\"data\", \"ES\", \"dev.in\")\n",
    "es_dev_out_path = os.path.join(\"data\", \"ES\", \"dev.out\")\n",
    "es_dev_p1_out_path = os.path.join(\"data\", \"ES\", \"dev.p1.out\")\n",
    "es_dev_p2_out_path = os.path.join(\"data\", \"ES\", \"dev.p2.out\")\n",
    "es_dev_p3_out_path = os.path.join(\"data\", \"ES\", \"dev.p3.out\")\n",
    "es_dev_p4_out_path = os.path.join(\"data\", \"ES\", \"dev.p4.out\")\n",
    "es_test_in_path = os.path.join(\"data\", \"ES-test\", \"test.in\")\n",
    "es_test_out_path = os.path.join(\"data\", \"ES-test\", \"test.out\")\n",
    "ru_train_path = os.path.join(\"data\", \"RU\", \"train\")\n",
    "ru_dev_in_path = os.path.join(\"data\", \"RU\", \"dev.in\")\n",
    "ru_dev_out_path = os.path.join(\"data\", \"RU\", \"dev.out\")\n",
    "ru_dev_p1_out_path = os.path.join(\"data\", \"RU\", \"dev.p1.out\")\n",
    "ru_dev_p2_out_path = os.path.join(\"data\", \"RU\", \"dev.p2.out\")\n",
    "ru_dev_p3_out_path = os.path.join(\"data\", \"RU\", \"dev.p3.out\")\n",
    "ru_dev_p4_out_path = os.path.join(\"data\", \"RU\", \"dev.p4.out\")\n",
    "ru_test_in_path = os.path.join(\"data\", \"RU-test\", \"test.in\")\n",
    "ru_test_out_path = os.path.join(\"data\", \"RU-test\", \"test.out\")\n",
    "\n",
    "\n",
    "# Define constant variables\n",
    "N = 7\n",
    "O, BPOS, IPOS, BNEU, INEU, BNEG, INEG = 0, 1, 2, 3, 4, 5, 6\n",
    "label_to_id = {\"O\": O,\n",
    "          \"B-positive\": BPOS,\n",
    "          \"I-positive\": IPOS,\n",
    "          \"B-neutral\": BNEU,\n",
    "          \"I-neutral\": INEU,\n",
    "          \"B-negative\": BNEG,\n",
    "          \"I-negative\": INEG,}\n",
    "id_to_label = [\"O\", \"B-positive\", \"I-positive\", \"B-neutral\", \"I-neutral\", \"B-negative\", \"I-negative\"]\n",
    "\n",
    "# Initialise a random number generator with a fixed seed for reproducible results and deterministic behavior\n",
    "rng = np.random.default_rng(1004519 + 1004103 + 1004555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dev.in data\n",
    "def read_dev_in_data(filepath):\n",
    "    results = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        sentence = []\n",
    "        for line in lines:\n",
    "            if line.strip() != '':\n",
    "                sentence.append(line.strip())  # add zero if meet unkown token\n",
    "            else:\n",
    "                results.append(sentence.copy())\n",
    "                sentence = []\n",
    "    return results\n",
    "\n",
    "# Read training data\n",
    "def read_training_data(filepath):\n",
    "    results = []\n",
    "    vocab = {'': 0}\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for line in lines:\n",
    "            if len(line.strip().rsplit(\" \", 1)) == 2:\n",
    "                token, label = line.strip().rsplit(\" \", 1)\n",
    "                if token not in vocab:\n",
    "                    vocab[token] = len(vocab)\n",
    "                tokens.append(vocab[token])\n",
    "                labels.append(label_to_id[label])\n",
    "            else:\n",
    "                results.append((tokens.copy(), labels.copy()))\n",
    "                tokens = []\n",
    "                labels = []\n",
    "    return results, vocab\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix.get(w, 0) for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, bidirectional=False)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG = 'ru'\n",
    "\n",
    "if LANG == 'es':\n",
    "    # load training data:\n",
    "    training_data, word_to_id = read_training_data(es_train_path)\n",
    "    # print(word_to_id)\n",
    "elif LANG == 'ru':\n",
    "    # load training data:\n",
    "    training_data, word_to_id = read_training_data(ru_train_path)\n",
    "    # print(word_to_id)\n",
    "\n",
    "EMBEDDING_DIM = 16\n",
    "HIDDEN_DIM = 16\n",
    "NUM_LAYERS = 4\n",
    "# With the above:\n",
    "# ES: 30 epochs\n",
    "# Loss: 0.054697051644325256\n",
    "# Entity F: 0.5746\n",
    "# Sentiment F: 0.4474\n",
    "# RU: 45 epochs\n",
    "# Loss: 0.029415829107165337\n",
    "# Entity F: 0.5191\n",
    "# Sentiment F: 0.3788"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMTagger(\n",
       "  (word_embeddings): Embedding(8329, 16)\n",
       "  (lstm): LSTM(16, 16, num_layers=4)\n",
       "  (hidden2tag): Linear(in_features=16, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, len(word_to_id), len(label_to_id))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# with torch.no_grad():\n",
    "#     inputs = torch.tensor(training_data[0][0], dtype=torch.long)\n",
    "#     tag_scores = model(inputs)\n",
    "#     print(tag_scores)\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0\n",
      "0.029119115322828293\n",
      "epoch  1\n",
      "0.034608807414770126\n",
      "epoch  2\n",
      "0.031016912311315536\n",
      "epoch  3\n",
      "0.03697674721479416\n",
      "epoch  4\n",
      "0.030748898163437843\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "for epoch in range(30):  # again, normally you would NOT do 300 epochs, it is toy data   \n",
    "    print(\"epoch \", epoch)\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = torch.tensor(sentence, dtype=torch.long).type(torch.cuda.LongTensor)\n",
    "        targets = torch.tensor(tags, dtype=torch.long).type(torch.cuda.LongTensor)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss.item())\n",
    "    if s == 0 and loss.item() < 0.01:\n",
    "        s = 1\n",
    "    elif s == 1 and loss.item() > 0.05:\n",
    "        break\n",
    "\n",
    "# See what the scores are after training\n",
    "# with torch.no_grad():\n",
    "#     inputs = prepare_sequence(test_data[0], word_to_id).type(torch.cuda.LongTensor)\n",
    "#     tag_scores = model(inputs)\n",
    "\n",
    "#     print(tag_scores)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running on dev.in\n",
    "if LANG == 'es':\n",
    "    test_data = read_dev_in_data(es_dev_in_path)\n",
    "    path = es_dev_p4_out_path\n",
    "elif LANG == 'ru':\n",
    "    test_data = read_dev_in_data(ru_dev_in_path)\n",
    "    path = ru_dev_p4_out_path\n",
    "\n",
    "with torch.no_grad():\n",
    "    with open(path, \"w+\", encoding=\"utf-8\") as file:\n",
    "        for sentence in test_data:\n",
    "            inputs = prepare_sequence(sentence, word_to_id).type(torch.cuda.LongTensor)\n",
    "            tag_scores = np.array(model(inputs).cpu())\n",
    "            pred_labels = np.argmax(tag_scores, axis=1)\n",
    "            for i in range(len(sentence)):\n",
    "                file.write(\"{} {}\\n\".format(sentence[i], id_to_label[pred_labels[i]]))\n",
    "            file.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running on test.in\n",
    "if LANG == 'es':\n",
    "    test_data = read_dev_in_data(es_test_in_path)\n",
    "    path = es_test_out_path\n",
    "elif LANG == 'ru':\n",
    "    test_data = read_dev_in_data(ru_test_in_path)\n",
    "    path = ru_test_out_path\n",
    "\n",
    "with torch.no_grad():\n",
    "    with open(path, \"w+\", encoding=\"utf-8\") as file:\n",
    "        for sentence in test_data:\n",
    "            inputs = prepare_sequence(sentence, word_to_id).type(torch.cuda.LongTensor)\n",
    "            tag_scores = np.array(model(inputs).cpu())\n",
    "            pred_labels = np.argmax(tag_scores, axis=1)\n",
    "            for i in range(len(sentence)):\n",
    "                file.write(\"{} {}\\n\".format(sentence[i], id_to_label[pred_labels[i]]))\n",
    "            file.write(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a542ca580817fc4dd55327026e074e2fa0cd470fc5dee9350c2d8b13822db8d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
